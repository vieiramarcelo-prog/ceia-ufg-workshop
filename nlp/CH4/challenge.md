# üìñ README: Teoria e Desafios (RAG Educacional)

Bem-vindo √† documenta√ß√£o te√≥rica do nosso reposit√≥rio de Retrieval-Augmented Generation (RAG). Este documento tem como objetivo explicar os conceitos por tr√°s do c√≥digo que voc√™ executou e propor desafios reais para evoluir essa arquitetura b√°sica para um n√≠vel de produ√ß√£o.

## üß† A Teoria: Como nosso RAG funciona?

O padr√£o RAG foi criado para resolver dois grandes problemas dos Modelos de Linguagem (LLMs): **alucina√ß√µes** (inventar fatos) e a **falta de conhecimento atualizado/privado**. Em vez de depender apenas do que o modelo aprendeu no treinamento, n√≥s "pesquisamos" a resposta antes em uma base confi√°vel e entregamos essa pesquisa para o LLM ler e formular a resposta final.

Nosso reposit√≥rio divide esse fluxo em quatro pilares fundamentais:

### 1. Ingest√£o e Representa√ß√£o Sem√¢ntica (Embeddings)

Computadores n√£o entendem palavras, entendem n√∫meros. Para que o nosso sistema saiba que "cachorro" e "c√£o" s√£o conceitos pr√≥ximos, transformamos nossos textos (o dataset SQuAD) em **Embeddings**.

* **O que fizemos:** Usamos o modelo `all-MiniLM-L6-v2` (pequeno e r√°pido para CPU) para converter cada par√°grafo de texto em um vetor de 384 dimens√µes.

### 2. O Banco de Dados Vetorial (Qdrant)

Bancos de dados tradicionais (SQL) buscam por palavras-chave exatas. Bancos vetoriais buscam por **proximidade geom√©trica**.

* **O que fizemos:** Subimos uma inst√¢ncia do Qdrant via Docker e salvamos nossos vetores l√°. Quando o usu√°rio faz uma pergunta, transformamos a pergunta em vetor e pedimos ao Qdrant: *"Me d√™ os 3 vetores mais pr√≥ximos (similares) a este aqui, usando a m√©trica de Dist√¢ncia de Cosseno"*.

### 3. Avalia√ß√£o de Recupera√ß√£o (M√©tricas)

Em um sistema RAG, se a busca falhar, o LLM vai falhar (ou alucinar). Por isso, precisamos medir a qualidade do nosso motor de busca usando m√©tricas cl√°ssicas de Recupera√ß√£o de Informa√ß√£o (IR):

* **Recall@k:** Dos documentos que importavam, quantos conseguimos trazer no nosso Top-K?
* **Precision@k:** Dos documentos que trouxemos no Top-K, quantos realmente eram √∫teis?
* **MRR (Mean Reciprocal Rank):** Qu√£o no topo da lista estava o primeiro resultado correto?
* **NDCG:** Avalia a qualidade do ranqueamento como um todo, penalizando resultados corretos que aparecem muito para o final da lista.

### 4. Gera√ß√£o Aumentada (LLM)

A √∫ltima etapa √© a s√≠ntese. Pegamos os textos brutos que o Qdrant retornou e os empacotamos em um *Prompt* junto com a pergunta original do usu√°rio.

* **O que fizemos:** Criamos uma instru√ß√£o simples mandando o modelo atuar como um assistente, ler o contexto fornecido e responder √† pergunta sem inventar dados.

---

## üöÄ Desafios para os Alunos: Evoluindo o Sistema

A vers√£o atual deste reposit√≥rio √© intencionalmente ing√™nua. Ela funciona muito bem para textos curtos (como os par√°grafos do SQuAD), mas falharia em cen√°rios do mundo real (ex: ler PDFs inteiros, contratos de 50 p√°ginas, etc.).

Sua miss√£o √© escolher um (ou mais) dos desafios abaixo e implementar no c√≥digo-fonte!

### N√≠vel 1: Melhorias Fundamentais

* **Desafio 1: Implementar *Chunking* (Fatiamento de Texto).** Atualmente, inserimos o par√°grafo inteiro no banco. E se os documentos tivessem 10 p√°ginas? O LLM estouraria o limite de tokens.
* *Sua tarefa:* Use bibliotecas como LangChain ou LlamaIndex para fatiar textos longos em peda√ßos de tamanho fixo (ex: 500 caracteres) com uma sobreposi√ß√£o (*overlap*) de 50 caracteres para n√£o perder o contexto entre as quebras.

* **Desafio 2: Engenharia de Prompt Avan√ßada.**
Nosso prompt atual √© extremamente b√°sico.
* *Sua tarefa:* Melhore o prompt no script `03_rag.py`. Adicione regras estritas (ex: *"Responda sempre em bullet points"*, *"Se a resposta n√£o estiver no texto, responda EXATAMENTE: 'Informa√ß√£o n√£o encontrada na base'"*). Adicione exemplos (*Few-Shot Prompting*) dentro da instru√ß√£o.

### N√≠vel 2: Otimiza√ß√£o de Busca

* **Desafio 3: Busca H√≠brida (Hybrid Search).**
A busca puramente sem√¢ntica √†s vezes ignora nomes pr√≥prios ou siglas espec√≠ficas.
* *Sua tarefa:* Pesquise sobre *Sparse Vectors* (como o BM25). Tente configurar o Qdrant para aceitar tanto a busca por embeddings (densa) quanto a busca por palavras-chave (esparsa) ao mesmo tempo.

* **Desafio 4: Adicionar um Re-Ranker (Cross-Encoder).**
Buscar muitos documentos √© r√°pido, mas impreciso.
* *Sua tarefa:* Traga 10 resultados do Qdrant usando o modelo MiniLM r√°pido, mas adicione uma etapa intermedi√°ria usando um modelo mais pesado (Cross-Encoder, ex: `cross-encoder/ms-marco-MiniLM-L-6-v2`) para reordenar esses 10 resultados com alta precis√£o antes de enviar os 3 melhores para o LLM. Avalie como isso afeta o NDCG no script `02_metricas.py`!

### N√≠vel 3: Arquitetura e Engenharia de Software

* **Desafio 5: Rodar 100% Local (Open Source).**
Atualmente o reposit√≥rio consome a API paga da OpenAI.
* *Sua tarefa:* Substitua a chamada da OpenAI por uma requisi√ß√£o local. Use ferramentas como o **Ollama** ou **LM Studio** para rodar um modelo menor na sua m√°quina (ex: `Llama-3-8B`, `Phi-3`, ou `Gemma`) e consuma a API local dele no script.

* **Desafio 6: Streaming de Resposta na API.**
O FastAPI atual espera o LLM gerar o texto inteiro antes de devolver para o usu√°rio.
* *Sua tarefa:* Modifique o endpoint `/rag` para utilizar *Server-Sent Events (SSE)* ou WebSockets, fazendo a resposta aparecer palavra por palavra na tela do usu√°rio, igual ao ChatGPT.
