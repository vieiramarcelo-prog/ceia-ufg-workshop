# CH2 - Qdrant (Vector DB) & LLMs Locais

Bem-vindo ao **CapÃ­tulo 2** do Workshop MLOps!

O foco deste capÃ­tulo Ã© **Infraestrutura para IA Generativa**. Vamos sair das APIs prontas (OpenAI) e aprender a rodar nossa prÃ³pria stack de inteligÃªncia.

## ğŸ¯ Objetivos de Aprendizado

1. **Qdrant**: Entender o que Ã© um Banco Vetorial, para que serve e como integrÃ¡-lo em uma aplicaÃ§Ã£o Python.
2. **LLMs Locais**: Aprender a servir modelos Open Source (Llama, Qwen, Mistral) usando Docker, sem depender de nuvem.

## ğŸ“‚ O LaboratÃ³rio: `practice/`

Para demonstrar essas tecnologias, criamos uma aplicaÃ§Ã£o prÃ¡tica (um Chatbot MÃ©dico com RAG) que usa o **Qdrant** como memÃ³ria e um **LLM Local** como cÃ©rebro.

ğŸ‘‰ **[ACESSAR O GUIA DA PRÃTICA](./practice/README.md)**

---

## ğŸ› ï¸ Stack TecnolÃ³gica

* **Qdrant**: Escolhido por ser open-source, muito rÃ¡pido e fÃ¡cil de subir com Docker.
* **vLLM / Llama.cpp**: PadrÃµes da indÃºstria para servir modelos LLM com alta performance.
* **Docker Compose**: Para subir toda essa infraestrutura complexa com um Ãºnico comando.

## ğŸš¦ Quick Start

```bash
cd practice
docker compose up --build
```

Acesse: **<http://localhost>**

---

> DÃºvidas? Consulte o [README detalhado da prÃ¡tica](./practice/README.md).
