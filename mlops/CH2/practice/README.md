# üè• Medical RAG - Pr√°tica de Qdrant & vLLM

Bem-vindo √† pr√°tica do **Cap√≠tulo 2 (CH2)**!

O objetivo deste laborat√≥rio √© dominar duas tecnologias fundamentais para IA moderna: **Bancos Vetoriais (Qdrant)** e **Infer√™ncia de LLMs (Llama.cpp/vLLM)**.

Para isso, constru√≠mos uma aplica√ß√£o de demonstra√ß√£o: um **Assistente M√©dico com RAG**.

![Badge](https://img.shields.io/badge/Focus-Vector%20Database%20%26%20Local%20LLM-blueviolet)

---

## üéØ O que voc√™ vai aprender?

O "Chat M√©dico" √© apenas o meio para aprendermos o fim:

1. **Qdrant (Vector Database)**:
    * Como armazenar texto em formato de n√∫meros (embeddings).
    * Como fazer buscas por *significado* (Busca Sem√¢ntica) e n√£o por palavras-chave.
2. **LLM Local (vLLM / Llama.cpp)**:
    * Como rodar uma Intelig√™ncia Artificial (como o Qwen2.5) no **seu pr√≥prio servidor/PC**, sem pagar API da OpenAI.
    * Entender o custo computacional e lat√™ncia.

---

## üöÄ Como Rodar

Pr√©-requisito: **Docker Desktop** instalado.

1. **Limpar ambiente anterior**:

    ```bash
    docker compose down -v
    ```

2. **Iniciar a Aplica√ß√£o**:

    ```bash
    docker compose up --build
    ```

    > ‚òï **Aguarde**: O sistema baixar√° o modelo de IA (~1.5GB).

3. **Acessar**:
    üëâ **[http://localhost](http://localhost)**

---

## ÔøΩ Explorando as Tecnologias

Use a aplica√ß√£o para "ver" o Qdrant e o LLM trabalhando:

### 1. Testando o Qdrant (A Mem√≥ria)

1. V√° na aba **"Base de Conhecimento"** e envie um PDF ou texto.
    * *O que acontece?* O texto √© quebrado, convertido em vetores pelo `FastEmbed` e indexado no **Qdrant**.
2. V√° no Chat e pergunte algo sobre o texto.
3. Olhe o **Painel de Debug (Direita)**:
    * Veja o **"Contexto Recuperado"**. Esses s√£o os trechos que o **Qdrant** achou mais similares matematicamente √† sua pergunta.

### 2. Testando o LLM Local (O C√©rebro)

1. Ainda no Painel de Debug, veja o **"Prompt Constru√≠do"**.
    * N√≥s "colamos" o texto do Qdrant dentro do prompt do modelo.
2. A resposta que aparece no chat √© gerada 100% localmente pelo container `ch2-llm`.

---

## üèóÔ∏è Arquitetura: Quem faz o qu√™?

### üß† LLM Service (`ch2-llm`)

* **O que √©**: Um servidor Python rodando `Llama.cpp`.
* **Papel**: Substitui o GPT-4. Recebe texto e completa o texto.
* **Modelo**: Usamos o `Qwen2.5-1.5B-Instruct` (leve e r√°pido para CPU).

### üíæ Qdrant (`ch2-qdrant`)

* **O que √©**: Um banco de dados especializado em vetores de alta dimens√£o.
* **Papel**: √â a "mem√≥ria de longo prazo". Permite que o LLM "leia" documentos que ele n√£o conhecia durante o treinamento.

### ‚öôÔ∏è API (`ch2-api`)

* **Papel**: O orquestrador.
    1. Recebe a pergunta.
    2. Vai no **Qdrant** buscar ajuda.
    3. Manda tudo pro **LLM**.
    4. Devolve pro usu√°rio.

---

## ‚ö†Ô∏è Dica de Estudo

Se voc√™ quer ver como conectamos o Python ao Qdrant, abra `app/services.py` e procure a classe `VectorDbService`. L√° est√° o c√≥digo cru de conex√£o e busca.
