# CH3 â€” PrÃ¡tica: Document Q&A API

API de perguntas e respostas sobre documentos construÃ­da com **FastAPI**, **LangChain** e **ChromaDB**. O usuÃ¡rio faz upload de PDFs ou arquivos de texto, e a API indexa o conteÃºdo num banco vetorial. A partir daÃ­, perguntas em linguagem natural sÃ£o respondidas via pipeline RAG (Retrieval-Augmented Generation) usando **GPT-4o-mini**, com fallback automÃ¡tico para **Google Gemini** caso a OpenAI nÃ£o esteja disponÃ­vel. Todo o fluxo Ã© observado em tempo real pelo **Arize Phoenix**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VocÃª faz upload de um PDF                          â”‚
â”‚       â†“                                             â”‚
â”‚  LangChain divide em chunks â†’ embeddings (OpenAI ou Google) â”‚
â”‚       â†“                                             â”‚
â”‚  ChromaDB armazena os vetores                       â”‚
â”‚       â†“                                             â”‚
â”‚  VocÃª faz uma pergunta                              â”‚
â”‚       â†“                                             â”‚
â”‚  API recupera os 4 chunks mais relevantes           â”‚
â”‚       â†“                                             â”‚
â”‚  GPT-4o-mini (ou Gemini) gera a resposta            â”‚
â”‚       â†“                                             â”‚
â”‚  Phoenix registra tudo: inputs, outputs, latÃªncia   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## PrÃ©-requisitos

| Ferramenta | VersÃ£o mÃ­nima |
|------------|---------------|
| Docker | 24+ |
| Docker Compose | v2 |
| OpenAI API key | â€” |
| Google AI API key _(opcional)_ | â€” |

---

## Como Rodar

Existem dois modos: **Docker Compose** (recomendado para este capÃ­tulo) e **Monolito** (serÃ¡ usado no CH4).

### Docker Compose â€” recomendado para o CH3

Cada serviÃ§o roda em seu prÃ³prio container. Ã‰ o modo que melhor ilustra os conceitos de MLOps deste capÃ­tulo: separaÃ§Ã£o de responsabilidades, health checks, volumes e rede Docker.

```bash
# 1. Crie o arquivo .env na raiz do projeto
echo "OPENAI_API_KEY=sk-..."  > .env
echo "GOOGLE_API_KEY=AIza..." >> .env   # opcional â€” ativa o fallback Gemini

# 2. Suba todos os serviÃ§os
docker compose up --build -d

# 3. Acompanhe os logs
docker compose logs -f api
```

Aguarde os health checks passarem (~15 s):

```bash
docker compose ps
```

| ServiÃ§o | URL |
|---------|-----|
| API (Swagger) | http://localhost:8000/docs |
| Streamlit UI | http://localhost:8501 |
| Phoenix (traces) | http://localhost:6006 |
| ChromaDB | http://localhost:8001 |
| DocumentaÃ§Ã£o | http://localhost:8080 |

---

### Monolito â€” usado no CH4

Todos os serviÃ§os rodam dentro de um Ãºnico container gerenciado pelo `supervisord`, com ChromaDB embarcado. Este modo Ã© o ponto de partida da prÃ¡tica do prÃ³ximo capÃ­tulo.

```bash
export OPENAI_API_KEY="sk-..."
export GOOGLE_API_KEY="AIza..."   # opcional

bash monolit/run.sh
```

---

## Uso rÃ¡pido via cURL

```bash
# 1. Obter token JWT
TOKEN=$(curl -s -X POST http://localhost:8000/auth/login \
  -d "username=admin&password=changeme" | jq -r .access_token)

# 2. Fazer upload de um documento
curl -s -X POST http://localhost:8000/documents \
  -H "Authorization: Bearer $TOKEN" \
  -F "files=@meu_documento.pdf"

# 3. Fazer uma pergunta
curl -s -X POST http://localhost:8000/rag/query \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"question": "Qual o assunto principal do documento?"}' | jq
```

A resposta inclui o campo `provider` indicando se foi `"openai"` ou `"gemini"` que respondeu.

---

## Credenciais padrÃ£o

| Campo | Valor |
|-------|-------|
| UsuÃ¡rio | `admin` |
| Senha | `changeme` |

> Altere via variÃ¡vel de ambiente `APP_USER=usuario:senha` antes de subir o container.

---

## VariÃ¡veis de Ambiente principais

| VariÃ¡vel | ObrigatÃ³ria | DescriÃ§Ã£o |
|----------|-------------|-----------|
| `OPENAI_API_KEY` | SimÂ¹ | Chave da OpenAI |
| `GOOGLE_API_KEY` | SimÂ¹ | Chave Google AI â€” embeddings e fallback Gemini |
| `GOOGLE_MODEL` | NÃ£o | Modelo Gemini (padrÃ£o: `gemini-2.0-flash`) |

> Â¹ Pelo menos uma das duas chaves Ã© obrigatÃ³ria. Se ambas estiverem definidas, **OpenAI Ã© preferida** tanto para embeddings quanto para geraÃ§Ã£o de resposta (Gemini fica como fallback automÃ¡tico).
| `OPENAI_MODEL` | NÃ£o | Modelo OpenAI (padrÃ£o: `gpt-4o-mini`) |
| `APP_USER` | NÃ£o | Credenciais `usuario:senha` (padrÃ£o: `admin:changeme`) |
| `SECRET_KEY` | NÃ£o | Chave JWT â€” gere com `openssl rand -hex 32` |

---

---

## ðŸŽ¯ Desafios do Workshop

O projeto jÃ¡ estÃ¡ funcional. Os desafios abaixo propÃµem extensÃµes reais â€” do tipo que aparece no dia a dia de quem mantÃ©m APIs de ML em produÃ§Ã£o.

> **Dica:** Leia o cÃ³digo em `main.py` antes de comeÃ§ar. Ele tem ~380 linhas e estÃ¡ organizado em seÃ§Ãµes comentadas.

---

### NÃ­vel 1 â€” Explorar (faÃ§a isso primeiro)

**Objetivo:** Entender o sistema antes de mexer nele.

1. Suba o projeto e acesse o Swagger em `http://localhost:8000/docs`
2. Autentique-se com `admin / changeme` e obtenha um token
3. FaÃ§a upload de qualquer PDF que vocÃª tenha (ou baixe um)
4. FaÃ§a uma pergunta sobre o documento e observe a resposta
5. Abra o Phoenix em `http://localhost:6006` e encontre o trace da sua pergunta
6. No trace, verifique: qual foi o prompt enviado ao LLM? Quantos tokens foram usados?

---

### NÃ­vel 2 â€” Modificar

Escolha **um** dos desafios abaixo:

#### 2-A: Adicionar o campo `chunk_count` na resposta do upload

Atualmente o endpoint `POST /documents` retorna apenas os nomes dos arquivos:
```json
{ "documents": ["relatorio.pdf"] }
```

Modifique para retornar tambÃ©m quantos chunks foram indexados:
```json
{ "documents": ["relatorio.pdf"], "chunk_count": 42 }
```

**Dica:** A funÃ§Ã£o `_ingest_file` jÃ¡ retorna `len(chunks)`. VocÃª precisa capturar esse valor no endpoint e expÃ´-lo no modelo Pydantic.

---

#### 2-B: Tornar o nÃºmero de chunks recuperados configurÃ¡vel

Atualmente o retriever sempre busca `k=4` chunks. Permita que o usuÃ¡rio passe esse valor na requisiÃ§Ã£o:

```json
POST /rag/query
{ "question": "O que Ã© MLOps?", "top_k": 8 }
```

**Dica:** Adicione o campo `top_k: int = 4` ao modelo `QueryRequest` e passe-o para `search_kwargs={"k": top_k}` na funÃ§Ã£o `_run_rag_query`.

---

#### 2-C: Endpoint para apagar um documento

Implemente `DELETE /documents/{filename}` que:
1. Remove o arquivo fÃ­sico de `UPLOAD_DIR`
2. Remove os chunks correspondentes do ChromaDB

```bash
curl -X DELETE http://localhost:8000/documents/relatorio.pdf \
  -H "Authorization: Bearer $TOKEN"
```

**Dica:** No ChromaDB vocÃª pode usar `collection.delete(where={"source": str(path)})` para remover chunks por metadado.

---

### NÃ­vel 3 â€” Construir

Desafios mais abertos, sem soluÃ§Ã£o Ãºnica:

#### 3-A: Testar o fallback Gemini

Configure `GOOGLE_API_KEY` com uma chave vÃ¡lida e force o fallback simulando uma falha na OpenAI. Uma forma simples: passe uma `OPENAI_API_KEY` invÃ¡lida e uma `GOOGLE_API_KEY` vÃ¡lida.

Observe no Phoenix se o trace mostra a tentativa com OpenAI e o retry com Gemini. O campo `provider` na resposta deve retornar `"gemini"`.

**Pergunta para reflexÃ£o:** Em produÃ§Ã£o, alÃ©m de fallback de LLM, o que mais vocÃª protegeria com retry/fallback nesse pipeline?

---

#### 3-B: Health check inteligente

O endpoint `/health` atual retorna apenas `{"status": "ok"}` sem verificar nada. Melhore-o para que reporte o estado real dos serviÃ§os dependentes:

```json
{
  "status": "ok",
  "chromadb": "ok",
  "documents_indexed": 127,
  "openai_key_configured": true,
  "gemini_key_configured": false
}
```

---

#### 3-C: Suporte a mÃºltiplas coleÃ§Ãµes

Hoje todos os documentos vÃ£o para a coleÃ§Ã£o `"documents"`. Adicione suporte a coleÃ§Ãµes nomeadas, permitindo que o usuÃ¡rio isole contextos diferentes:

```bash
# Upload para uma coleÃ§Ã£o especÃ­fica
POST /documents?collection=juridico

# Query em uma coleÃ§Ã£o especÃ­fica
POST /rag/query
{ "question": "...", "collection": "juridico" }
```

**Ponto de atenÃ§Ã£o:** A coleÃ§Ã£o usada no upload precisa ser a mesma usada na query (os embeddings precisam estar no mesmo espaÃ§o vetorial).

---

### NÃ­vel 4 â€” Pesquisa

Para quem terminar tudo e quiser algo para pensar:

> O pipeline atual usa `text-embedding-ada-002` da OpenAI para gerar os embeddings â€” tanto no upload quanto na query. Isso significa que se vocÃª mudar o modelo de embedding, precisarÃ¡ re-indexar todos os documentos.
>
> **Desafio:** Como vocÃª arquitetaria um sistema que permita migrar modelos de embedding sem downtime e sem perder os documentos jÃ¡ indexados? Esboce a soluÃ§Ã£o (pode ser em texto, diagrama ou pseudo-cÃ³digo).

---

## Estrutura do Projeto

```
pratica/
â”œâ”€â”€ main.py                  # FastAPI â€” toda a lÃ³gica da API
â”œâ”€â”€ pyproject.toml           # DependÃªncias (gerenciadas com uv)
â”œâ”€â”€ docker-compose.yml       # Stack multi-container
â”œâ”€â”€ Dockerfile               # Imagem da API
â”œâ”€â”€ .env.example             # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ streamlit_app/
â”‚   â””â”€â”€ app.py               # Interface web (Streamlit)
â”œâ”€â”€ vectordb/
â”‚   â””â”€â”€ Dockerfile           # Imagem do ChromaDB
â”œâ”€â”€ docs/                    # Fonte da documentaÃ§Ã£o (MkDocs)
â”œâ”€â”€ mkdocs.yml               # ConfiguraÃ§Ã£o do MkDocs
â””â”€â”€ monolit/                 # VersÃ£o single-container
    â”œâ”€â”€ main.py              # Mesma API, ChromaDB embarcado
    â”œâ”€â”€ Dockerfile           # Tudo num container sÃ³ (supervisord)
    â”œâ”€â”€ supervisord.conf     # Gerenciador de processos
    â””â”€â”€ run.sh               # Script de build + run
```
